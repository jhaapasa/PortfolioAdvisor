# Architecture Overview

This document explains the overall design of PortfolioAdvisor: the data flow through its agents, the state management approach, configuration options, and guidance for extending the system.

## Design Goals
- **Simplicity:** Reliable end-to-end portfolio ingestion and analysis with minimal setup
- **Modularity:** Clear boundaries between ingestion, parsing, resolution, and analysis stages
- **Extensibility:** Easy to add new data providers, analysis types, and agents
- **Observability:** Transparent state management and comprehensive logging
- **Testability:** Deterministic agent behavior with >80% code coverage

## High-Level Flow

The system follows a linear pipeline with two parallel fan-out/fan-in stages:

1. **CLI Initialization**
   - Parses arguments and environment variables
   - Builds application settings
   - Configures logging and LLM caching
   - Invokes the main analysis entrypoint

2. **Agent Orchestration** (LangGraph state machine)
   ```
   ingestion → planner → dispatch_parse → [parse_one × N] → join_after_parse →
   dispatch_resolve → [resolve_one × M] → join_after_resolve → analyst → END
   ```

3. **Output Generation**
   - Compiled analysis report (`analysis.md`)
   - Structured holdings data (`resolved_positions.json`)
   - Portfolio persistence (`output/portfolio/`)

### Agent Pipeline Stages

- **Ingestion:** Discovers and extracts plain text from input files
- **Planner:** Analyzes document structure and creates parsing strategy
- **Parse (fan-out):** LLM-powered extraction of holdings from each document in parallel
- **Resolve (fan-out):** Symbol resolution via Polygon.io for each holding in parallel
- **Analyst:** Generates narrative summary of portfolio composition and insights

## LangGraph State Contract

The graph state is a typed dictionary with well-defined channels for data flow between agents:

### Core State Fields
- **`settings`** — Application configuration (immutable)
- **`raw_docs`** — List of ingested documents with metadata (path, MIME type, extracted text)
- **`plan`** — Parsing strategy with rationale from the planner agent

### Additive Channels
These channels accumulate results across parallel fan-out operations:
- **`parsed_holdings`** — Candidate holdings extracted from documents (pre-resolution)
- **`resolved_holdings`** — Canonical holdings with validated symbols and instrument keys
- **`unresolved_entities`** — Holdings that failed symbol resolution with error reasons
- **`errors`** — Error messages from any agent

### Output Channels
- **`analysis`** — Final narrative report generated by the analyst agent

### State Update Semantics
- Additive channels use the `operator.add` reducer to merge results from parallel nodes
- Single-value fields use last-write-wins semantics
- All state updates are immutable; agents return new state dictionaries

## Agent Implementations

### Ingestion Agent (`agents/ingestion.py`)
**Purpose:** Extract structured text from heterogeneous input files

**Capabilities:**
- Discovers all files in `input_dir` recursively
- Supports multiple formats: TXT, MD, HTML, CSV, EML
- Applies 2 MiB hard cap per file for memory safety
- Filters OS artifacts (`.DS_Store`, hidden files)
- Extracts MIME type metadata for downstream processing

**Output:** Populates `raw_docs` state channel

### Planner Agent (`agents/planner.py`)
**Purpose:** Analyze document structure and create parsing strategy

**Capabilities:**
- Inspects document types and content structure
- Generates step-by-step parsing plan with rationale
- Identifies document-specific extraction strategies

**Output:** Populates `plan` state field

### Parser Agent (`agents/parser.py`)
**Purpose:** Extract holdings from documents using LLM-powered structured output

**Capabilities:**
- Fan-out: One parser instance per document (parallel execution)
- Prompts LLM to emit strict JSON matching `ParsedHoldingsResult` schema
- Automatic retry on validation errors (configurable via `PARSER_MAX_RETRIES`)
- Input truncation to `PARSER_MAX_DOC_CHARS` to manage token limits
- Schema-validated output with Pydantic models

**Output:** Accumulates results in `parsed_holdings` channel

### Resolver Agent (`agents/resolver.py` + `tools/symbol_resolver.py`)
**Purpose:** Resolve ticker symbols to canonical instrument identifiers

**Capabilities:**
- Fan-out: One resolver call per holding (parallel execution)
- Queries Polygon.io Ticker Search API for symbol validation
- Heuristic ranking: active status + preferred MIC + currency match
- Confidence threshold filtering (configurable)
- Offline mode: gracefully handles missing API key
- Generates canonical `InstrumentKey` format: `cid:asset_class:locale:mic:symbol`

**Output:** Populates `resolved_holdings` and `unresolved_entities` channels

### Analyst Agent (`agents/analyst.py`)
**Purpose:** Generate portfolio analysis report

**Capabilities:**
- Synthesizes insights from resolved holdings
- LLM-powered narrative generation
- Summarizes portfolio composition and allocation
- Identifies notable positions and concentrations

**Output:** Populates `analysis` state field

## Data Models

### Parsed Models (`models/parsed.py`)
Represents raw LLM extraction output before symbol resolution:

- **`ParsedHolding`** — Single holding with fields:
  - `ticker` — Raw ticker symbol from document
  - `quantity` — Number of shares/units
  - `description` — Asset description (optional)
  - `account` — Account identifier (optional)

- **`ParsedHoldingsResult`** — Container for parser output:
  - `holdings` — List of `ParsedHolding` objects
  - `metadata` — Extraction metadata (document name, parser version)

### Canonical Models (`models/canonical.py`)
Represents validated, normalized holdings after resolution:

- **`InstrumentKey`** — Globally unique identifier with format:
  ```
  cid:asset_class:locale:mic:symbol
  ```
  Example: `cid:stocks:us:xnys:AAPL`

- **`CanonicalHolding`** — Enriched holding record:
  - `instrument_key` — Canonical identifier
  - `ticker` — Resolved ticker symbol
  - `quantity` — Share count
  - `asset_class` — Asset type (stocks, crypto, etc.)
  - `locale` — Market locale (us, gb, etc.)
  - `mic` — Market Identifier Code (exchange)
  - `name` — Full instrument name
  - `currency` — Trading currency
  - `confidence` — Resolution confidence score (0.0–1.0)

## Configuration System

Settings are defined in `src/portfolio_advisor/config.py` using Pydantic for validation. Configuration is loaded from environment variables (via `.env` in development) and can be overridden with CLI flags.

### Configuration Categories

**Required Paths**
- `INPUT_DIR` / `--input-dir` — Portfolio data directory
- `OUTPUT_DIR` / `--output-dir` — Analysis output directory

**LLM Provider**
- `OPENAI_API_KEY` — API authentication (stub mode if absent)
- `OPENAI_MODEL` — Model selection (default: `gpt-4o-mini`)
- `OPENAI_BASE_URL` — Custom endpoint (optional)
- `TEMPERATURE` — Sampling temperature (default: `0.2`)
- `MAX_TOKENS` — Output token limit (optional)
- `REQUEST_TIMEOUT_S` — API timeout (default: `60`)

**Parser Tuning**
- `PARSER_MAX_RETRIES` — Retry limit for validation errors (default: `2`)
- `PARSER_MAX_DOC_CHARS` — Document truncation limit (default: `20000`)

**Symbol Resolution**
- `POLYGON_API_KEY` — Polygon.io authentication (offline mode if absent)
- `POLYGON_BASE_URL` — API endpoint override (optional)
- `POLYGON_TIMEOUT_S` — Request timeout (default: `30`)
- `RESOLVER_DEFAULT_LOCALE` — Fallback market locale (default: `us`)
- `RESOLVER_PREFERRED_MICS` — Comma-separated exchange codes (e.g., `XNYS,XNAS`)
- `RESOLVER_CONFIDENCE_THRESHOLD` — Minimum match score (default: `0.8`)

**Logging and Observability**
- `LOG_LEVEL` — Verbosity (DEBUG, INFO, WARNING, ERROR)
- `LOG_FORMAT` — Output format (plain, json)
- `VERBOSE` — Enable verbose agent logging
- `AGENT_PROGRESS` — Show LangGraph execution details

**Caching**
- `SKIP_LLM_CACHE` — Force cache bypass (writes still occur)
- Cache database: `./cache/langchain_cache.sqlite3`

### Stub Mode Behavior
When `OPENAI_API_KEY` is not provided, the system uses a stub LLM that returns placeholder responses. This enables testing and demos without API costs.

## Logging and Error Handling

### Logging Strategy
- Default level: `INFO` with plain text format
- JSON format available for structured log aggregation
- Library noise suppression: LangGraph/LangChain logs hidden by default
- Enable `AGENT_PROGRESS=1` to show detailed execution traces
- Sensitive data (API keys) never logged

### Error Handling Principles
- Fail fast with clear error messages
- Configuration errors detected at startup
- File I/O errors include file paths and context
- LLM validation errors include retry count and schema details
- Network errors include endpoint and timeout information
- All exceptions inherit from custom base classes for type-safe handling

## Extensibility Guide

### Adding a New Data Provider
1. Create client in `services/` (e.g., `alpha_vantage_client.py`)
2. Implement standard interface: `fetch_ticker_info(symbol: str) -> TickerInfo`
3. Update `SymbolResolver` in `tools/symbol_resolver.py` to use new provider
4. Add configuration settings to `config.py`
5. Update tests with mocked provider responses

### Adding a New Agent
1. Create agent function in `agents/` following the pattern:
   ```python
   def my_agent(state: dict) -> dict:
       # Process state and return updated state
       return {"my_field": result}
   ```
2. Update state schema in `graph.py` with new state keys
3. Add agent to graph topology using `.add_node()` and `.add_edge()`
4. Write unit tests with minimal state fixtures

### Adding a New Analysis Type
1. Create module in `stocks/` (e.g., `momentum.py`)
2. Implement computation function with clear inputs/outputs
3. Add to stock analysis graph in `graphs/stocks.py`
4. Update file-based database schema in `stock-analysis-plan.md`
5. Add integration tests with sample OHLC data

### Customizing LLM Prompts
- Parser prompts: `agents/parser.py` — Update `PARSER_SYSTEM_PROMPT` constant
- Analyst prompts: `agents/analyst.py` — Modify prompt templates
- Add version tracking to metadata when changing schemas

## Repository Structure

```
PortfolioAdvisor/
├── src/portfolio_advisor/       # Core application
│   ├── agents/                  # LangGraph agent nodes
│   │   ├── ingestion.py         # File discovery and text extraction
│   │   ├── parser.py            # LLM-powered holdings extraction
│   │   ├── planner.py           # Document analysis and strategy
│   │   ├── resolver.py          # Symbol resolution orchestration
│   │   ├── analyst.py           # Report generation
│   │   └── news_summary.py      # News aggregation and summarization
│   ├── graphs/                  # LangGraph pipeline definitions
│   │   ├── baskets.py           # Basket analysis workflow
│   │   └── stocks.py            # Stock data collection workflow
│   ├── models/                  # Pydantic data models
│   │   ├── canonical.py         # Post-resolution holdings
│   │   └── parsed.py            # Pre-resolution holdings
│   ├── services/                # External API clients
│   │   ├── polygon_client.py    # Polygon.io REST API wrapper
│   │   └── ollama_service.py    # Local LLM service (experimental)
│   ├── stocks/                  # Stock analysis modules
│   │   ├── analysis.py          # Returns, volatility, SMAs
│   │   ├── db.py                # File-based stock database
│   │   ├── news.py              # News fetching and extraction
│   │   ├── paths.py             # Database path management
│   │   ├── plotting.py          # Chart generation
│   │   └── wavelet.py           # Wavelet decomposition
│   ├── tools/                   # Reusable utilities
│   │   └── symbol_resolver.py   # Ticker resolution logic
│   ├── utils/                   # Helper functions
│   │   ├── fs.py                # Filesystem utilities
│   │   └── slug.py              # Path-safe name generation
│   ├── cli.py                   # Command-line interface
│   ├── config.py                # Settings and environment handling
│   ├── graph.py                 # Main portfolio analysis graph
│   ├── analyze.py               # Analysis entrypoint
│   └── llm.py                   # LLM client factory
├── tests/                       # Comprehensive test suite
│   ├── test_*_agent.py          # Agent unit tests
│   ├── test_*_graph.py          # Graph integration tests
│   └── test_*_integration.py    # End-to-end tests
├── docs/                        # Technical documentation
│   ├── Architecture.md          # This file
│   ├── stock-analysis-plan.md   # Stock data design
│   └── feature-design-*.md      # Feature specifications
├── scripts/                     # Development automation
│   ├── bootstrap                # Environment setup
│   ├── format                   # Code formatting (Black)
│   ├── lint                     # Linting (Ruff)
│   └── test                     # Test runner with coverage
├── input/                       # Sample portfolio files
├── output/                      # Generated analysis outputs
│   ├── portfolio/               # Portfolio state persistence
│   ├── baskets/                 # Basket analysis reports
│   └── stocks/                  # Stock database and charts
└── cache/                       # LLM response cache
```
